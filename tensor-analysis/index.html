<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Tensor Analysis - FreeMath13</title>
<meta name="description" content="Contents: what tensors are, basic tensor concepts, tensor operations, coordinate transformations, definition of tensors, ordinary and covariant differentiation.">
<meta name="author" content="George Makris">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script src="/mathjax.v1.js" defer></script>
<script src="/menu.v1.js" defer></script>
<script src="/back-to-top.v1.js" defer></script>
<script src="/apply-settings.v1.js" defer></script>
<script src="/colors.v1.js" defer></script>
<link rel="stylesheet" href="/style.v1.css">
<link rel="alternate" hreflang="en" href="https://freemath13.com/tensor-analysis/">
<link rel="alternate" hreflang="el" href="https://freemath13.com/tensor-analysis/gr/">
<link rel="canonical" href="https://freemath13.com/tensor-analysis/">
<link rel="alternate" hreflang="x-default" href="https://freemath13.com/tensor-analysis/">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.v1.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.v1.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.v1.png">
<link rel="manifest" href="/site.v1.webmanifest">
<meta property="og:site_name" content="FreeMath13">
<meta property="og:title" content="Tensor Analysis - FreeMath13">
<meta property="og:description" content="Contents: what tensors are, basic tensor concepts, tensor operations, coordinate transformations, definition of tensors, ordinary and covariant differentiation.">
<meta property="og:type" content="website">
<meta property="og:url" content="https://freemath13.com/tensor-analysis/">
</head>
<body>
<div class="start-of-page" aria-hidden="true"></div>
<a href="#intro" class="doNotStyle" id="accessibility-link">Skip To Main Content</a>
<header>
<div id="title"><h1>Tensor Analysis</h1></div>
<div class="dropdown-menu">
<button class="menu-button" aria-expanded="false" aria-controls="access-links" aria-label="Useful Links"><span></span><span></span><span></span></button>
<nav id="access-links" class="menu-links">
<h2>Useful Links</h2><ul>
<li><a href="/accessibility/" class="doNotStyle">Accessibility Settings</a></li>
<li><a href="/" class="doNotStyle">Homepage</a></li>
<li><a href="/library/" class="doNotStyle">Course Library</a></li>
<li><a href="/help/" class="doNotStyle">Help</a></li>
</ul>
</nav>
</div>
<div class="logo-container">
<img src="/android-chrome-512x512.v1.png" alt="">
<span>FreeMath13</span>
</div>
<hr>
<div id="settings">
<div id="settings_box1">
<h2>Language:</h2>
<p class="language">ENGLISH</p>
<hr>
<p class="center">Other languages:</p>
<a href="/tensor-analysis/gr/" class="language">ΕΛΛΗΝΙΚΑ</a>
</div>
<div id="settings_box2">
<h2>Modes:</h2>
<div class="center">
<button id="defaultMode">Default</button>
<button id="lightMode">Light Mode</button>
</div>
<div class="center">
<button id="darkMode">Dark Mode</button>
<button id="darkMode2">FreeMath13</button>
</div>
<div class="center">
<a href="/custom-mode/" id="customMode">Custom Mode</a>
</div>
</div>
</div>
<hr>
<h2>Contents</h2>
<nav><ul>
<li><a href="#intro">Introduction</a></li>
<li><h3>Prerequisite Knowledge</h3><ul>
<li><a href="#prerequisites">Prerequisite Courses</a></li>
<li><a href="#Kroneckers-Delta">Kronecker Delta</a></li>
<li><a href="#covectors">Linear Forms</a></li>
<li><a href="#dual-spaces">Dual Spaces</a></li></ul></li>
<li><h3>Basic Tensor Concepts</h3><ul>
<li><a href="#basic-concepts">What Tensors Are</a></li>
<li><a href="#order-of-tensors">Order Of Tensors</a></li>
<li><a href="#tensor-types">Types Of Tensors</a></li>
<li><a href="#Einsteins-convention">Einstein Convention</a></li></ul></li>
<li><h3>Tensor Operations</h3><ul>
<li><a href="#addition-and-subtraction">Addition And Subtraction</a></li>
<li><a href="#scalar-multiplication">Scalar Multiplication</a></li>
<li><a href="#multiplication">Multiplication</a></li>
<li><a href="#contraction">Contraction</a></li></ul></li>
<li><h3>Coordinate Transformations In Linear Coordinate Systems</h3><ul>
<li><a href="#intro-to-coordinates-transformation">Introduction</a></li>
<li><a href="#vector-coordinates-transformation">Transformation Of Vector Coordinates</a></li>
<li><a href="#covector-coordinates-transformation">Transformation Of Linear Form Coordinates</a></li>
<li><a href="#tensor-coordinates-transformation">Transformation Of The Coordinates Of An Arbitrary Tensor</a></li></ul></li>
<li><h3>Coordinate Transformations In Curvilinear Coordinate Systems</h3><ul>
<li><a href="#curvilinear-coordinates">Curvilinear Coordinates</a></li>
<li><a href="#tensor-coordinates-transformation-2">Transformation Of The Coordinates Of An Arbitrary Tensor</a></li></ul></li>
<li><h3>Definition Of Tensors</h3><ul>
<li><a href="#multilinear-maps">Multilinear Maps</a></li>
<li><a href="#tensor-product-spaces">Tensor Product Spaces</a></li>
<li><a href="#tensors-definition">Definition Of Tensors</a></li>
<li><a href="#how-to-check-if-something-is-a-tensor">How To Check Whether A Mathematical Object Is A Tensor</a></li></ul></li>
<li><h3>Metric Tensor</h3><ul>
<li><a href="#metric-tensor">Definition</a></li>
<li><a href="#examples-of-metric-tensor">Examples</a></li>
<li><a href="#uses-of-metric-tensor">Uses</a></li></ul></li>
<li><h3>Tensor Differentiation</h3><ul>
<li><a href="#ordinary-partial-differentiation">Ordinary Differentiation</a></li>
<li><a href="#covariant-differentiation">Covariant Differentiation</a></li></ul></li>
<li><h3>Tensor Form Of Well-Known Math Concepts</h3><ul>
<li><a href="#tensor-form-of-gradient">Gradient</a></li>
<li><a href="#tensor-form-of-divergence">Divergence</a></li>
<li><a href="#tensor-form-of-curl">Curl</a></li>
<li><a href="#tensor-form-of-Laplacian">Laplacian</a></li></ul></li>
</ul></nav>
<p>Sentences starting with an exclamation mark can be scrolled horizontally if they don't fit on the screen.</p>
<hr>
</header>
<main>
<h2 id="intro">Introduction</h2>
<p><b>Tensor Analysis</b> is one of those courses that can feel intimidating at first, yet quickly proves itself too powerful and too useful to overlook.</p>
<p>With applications ranging from differential geometry and general relativity to fluid mechanics and beyond, it has become an essential tool for mathematicians, physicists, and engineers alike.</p>
<p>Whether you are taking it as a required university course or exploring it out of pure curiosity, stay with us as we build a solid understanding of tensors from the ground up.</p>
<hr>
<h2 id="prerequisites">Prerequisite Knowledge</h2>
<p>To follow this course, you should have basic knowledge of linear algebra and calculus.</p>
<p>Everything you need is included in the courses:</p>
<ul>
<li><a href="/linear-algebra-I/">Linear Algebra <span class="consolas">I</span></a></li>
<li><a href="/calculus-III/">Calculus <span class="consolas">III</span></a></li>
</ul>
<p>A few additional concepts are also required, which we will now explain.</p>
<h3 id="Kroneckers-Delta">Kronecker Delta</h3>
<p>The Kronecker delta is defined as \(δ_{ij}=\begin{cases} 0 \text{ , if } i\neq j \\ 1 \text{ , if } i=j \end{cases}\)</p>
<p>Depending on the context, you may also see it written as \(δ^i_j\) or even \(δ^{ij}.\)</p>
<p>For example, if \(i,j\) take the values \(1,2:\)</p>
<ul>
<li>\(δ_{11}=δ_{22}=1\)</li>
<li>\(δ_{12}=δ_{21}=0\)</li>
</ul>
<h4>Generalized Kronecker Delta</h4>
<p>It is defined as:</p>
<p class="math-container">\(\quad ! \quad δ^{i_1\,i_2\,\dots\,i_n}_{j_1\,j_2\,\dots\,j_n}=\begin{cases} +1 \text{ , if } \{j_1,j_2,\dots,j_n\} \text{ is an even permutation of } \{i_1,i_2,\dots,i_n\} \\ -1 \text{ , if } \{j_1,j_2,\dots,j_n\} \text{ is an odd permutation of } \{i_1,i_2,\dots,i_n\} \\ 0 \text{ , if at least two of } j_1,j_2,\dots,j_n \text{ or at least two of } i_1,i_2,\dots,i_n \text{ have the same value.}\end{cases}\)</p>
<p>An even permutation means they differ in an even number of positions.</p>
<p>An odd permutation means they differ in an odd number of positions.</p>
<p>For example, if \(i_1,i_2,j_1,j_2\) take the values \(1,2:\)</p>
<ul>
<li>\(δ^{1\,1}_{1\,1}=δ^{1\,1}_{1\,2}=δ^{1\,1}_{2\,1}=δ^{1\,1}_{2\,2}=0\)</li>
<li>\(δ^{2\,2}_{1\,1}=δ^{2\,2}_{1\,2}=δ^{2\,2}_{2\,1}=δ^{2\,2}_{2\,2}=0\)</li>
<li>\(δ^{1\,2}_{1\,1}=δ^{1\,2}_{2\,2}=δ^{2\,1}_{1\,1}=δ^{2\,1}_{2\,2}=0\)</li>
<li>\(δ^{1\,2}_{1\,2}=δ^{2\,1}_{2\,1}=1\)</li>
<li>\(δ^{1\,2}_{2\,1}=δ^{2\,1}_{1\,2}=-1\)</li>
</ul>
<h3 id="covectors">Linear Forms</h3>
<p><b>Linear forms (or linear functionals, or covectors)</b> are linear maps \(ω:V\to\mathbb{R},\) where \(V\) is usually an \(n\)-dimensional vector space over \(\mathbb{R}.\)</p>
<p>That is, they are linear functions that take a vector as input and produce a real number.</p>
<p>In \(V=\mathbb{R}^n\) with Cartesian coordinates, their form is \(ω(x_1,\dots,x_n)=ω_1x_1+\dots+ω_nx_n,\) with \(ω_i\in\mathbb{R}.\)</p>
<p>The tuple \((ω_1,\dots,ω_n)\) is called the <b>coordinates or components</b> of the linear form.</p>
<p>For example, a linear form is \(ω:\mathbb{R}^2\to\mathbb{R}\) with formula \(ω(x,y)=x+y.\) Its coordinates are \((1,1).\)</p>
<h3 id="dual-spaces">Dual Spaces</h3>
<p>The set of all linear forms \(ω:V\to\mathbb{R},\) is denoted by \(V^*.\)</p>
<p>We define their addition and scalar multiplication as:</p>
<ul>
<li>\(+:V^*\to V^*\) with \((ω_1+ω_2)(\vec{v})=ω_1(\vec{v})+ω_2(\vec{v})\)</li>
<li>\(\cdot:\mathbb{R}\times V^*\to V^*\) with \((c\cdot ω)(\vec{v})=c\cdot ω(\vec{v})\)</li>
</ul>
<p>The triple \((V^*,+,\cdot)\) is called the <b>dual space</b> of the vector space \(V.\)</p>
<h4>Basis Of The Dual Space</h4>
<p>If \(B_V=\{e_1,\dots,e_n\}\) is a basis of \(V,\) then the basis of the dual space \(V^*\) is defined as \(B_{V^*}=\{e^1,\dots,e^n\}\) with \(e^i(e_j)=δ^i_j\)</p>
<p>The indices \(i\) in \(e^i\) are not exponents; they are indices that are written upstairs instead of downstairs.</p>
<p>We will see in the section on coordinate transformations why this happens.</p>
<h5>How We Find The Dual Basis</h5>
<p>The dual basis is not found on its own; it is found with respect to a basis of the vector space.</p>
<p>For example, in \(\mathbb{R}^2,\) one basis is \(B=\{(0,1),(1,0)\}.\)</p>
<p>That is, \(e_1=(0,1)\) and \(e_2=(1,0).\)</p>
<p>The dual basis with respect to \(B\) consists of two linear forms \(e^1,e^2:\mathbb{R}^2\to\mathbb{R}\) of the form \(e^1(x,y)=ax+by\) and \(e^2(x,y)=cx+dy\) respectively.</p>
<p>To find \(a,b,c,d,\) we use the definition of the dual basis. That is:</p>
<p class="math-container">\(\quad ! \quad e^1(e_1)=δ^1_1 \iff e^1(0,1)=1 \iff a\cdot 0+b\cdot 1=1 \iff b=1\)</p>
<p class="math-container">\(\quad ! \quad e^1(e_2)=δ^1_2 \iff e^1(1,0)=0 \iff a\cdot 1+b\cdot 0=0 \iff a=0\)</p>
<p class="math-container">\(\quad ! \quad e^2(e_1)=δ^2_1 \iff e^2(0,1)=0 \iff c\cdot 0+d\cdot 1=0 \iff d=0\)</p>
<p class="math-container">\(\quad ! \quad e^2(e_2)=δ^2_2 \iff e^2(1,0)=1 \iff c\cdot 1+d\cdot 0=1 \iff c=1\)</p>
<p>Therefore, the dual basis with respect to \(B\) is \(\{e^1,e^2\}\) with \(e^1(x,y)=y\) and \(e^2(x,y)=x\)</p>
<hr>
<h2 id="basic-concepts">Basic Tensor Concepts</h2>
<br>
<h3>What Tensors Are</h3>
<p>There is no point in giving the formal definition of tensors right away, since it would most likely confuse you more than it would help.</p>
<p>A simpler definition, which is used only to give you an idea of what tensors are and does not replace the formal one, is the following:</p>
<p>"Tensors are mathematical objects that are independent of the coordinate system used to describe them."</p>
<p>For example, a vector \(\vec{v}=(a,b)\) is an arrow in the plane.</p>
<p>If we change the coordinate system, its coordinates will change from \((a,b)\) to \((\tilde{a},\tilde{b}).\)</p>
<p>Its length and direction, however, will remain the same. That is, the vector remains the same geometrically.</p>
<p>For this reason, \(\vec{v},\) as well as all other vectors, are tensors.</p>
<p>In general, it is important to understand early on that tensors are not a specific collection of objects. Anything that satisfies the definition of a tensor, which we will see later in the course, is a tensor.</p>
<h4>Examples Of Tensors</h4>
<ul>
<li>Vectors: we explained why they are tensors.</li>
<li>Linear forms: they take a vector as input and return a number. This number is the same in every coordinate system.</li>
<li>Multilinear forms: similar to linear forms, but they take more than one vector as input.</li>
<li>Scalar functions: they take a point as input and return a number. This number is the same in every coordinate system.</li>
<li>Vector fields: they take a point as input and return a vector.</li>
<li>Integrals of scalar functions: they compute scalar quantities (area, volume, etc.), i.e. numbers that are the same in every coordinate system.</li>
<li>Gradient (nabla) of a scalar function: it is a vector field.</li>
<li>Divergence: it is a scalar function.</li>
<li>Curl in the plane: it is a scalar function.</li>
<li>Curl in space: it is a vector field.</li>
<li>And others.</li>
</ul>
<h3>Notation</h3>
<p>Depending on their type and on the vector space in which they are defined, tensors are denoted by a letter together with some indices.</p>
<p>For example, \(T^i,\) \(T_j,\) \(T^i_j,\) \(T^{ij}_k,\dots,\) where the indices \(i,j,k,\dots,\) take values from \(1\) up to a number \(n,\) which is the dimension of the vector space in which the tensor lives.</p>
<p>The number of indices is determined by the order of the tensor, which we will discuss shortly.</p>
<p>The reason an index appears upstairs or downstairs is related to how the coordinates transform, and this will be explained later in the corresponding section.</p>
<h3 id="order-of-tensors">Order Of Tensors</h3>
<p>Let \(V\) be an \(n-\)dimensional vector space over \(\mathbb{R}.\)</p>
<p>In this vector space, tensors of type \((p,q),\) where \(p\in\mathbb{N}\) is the number of upper indices and \(q\in\mathbb{N}\) is the number of lower indices, are of order \(p+q\) and have \(n^{p+q}\) components.</p>
<p>The components of tensors depend on the basis of \(V\) that we use and are either numbers or scalar functions.</p>
<p>For example, in the vector space \(\mathbb{R}^3\) using Cartesian coordinates, a tensor \(T^i_j\) with \(i,j=1,2,3:\)</p>
<ul>
<li>Is of type \((1,1),\) since it has one upper and one lower index.</li>
<li>Is of order \(1+1=2.\)</li>
<li>Has \(3^2=9\) components.</li>
</ul>
<p>Its components are denoted by:</p>
<p class="math-container">\(\quad ! \quad T^1_1,\,T^1_2,\,T^1_3,\,T^2_1,\,T^2_2,\,T^2_3,\,T^3_1,\,T^3_2,\,T^3_3\)</p>
<p>There is one for each combination of the indices \(i,j,\) which take values from \(1\) to \(3.\)</p>
<p>As mentioned before, these are either numbers or scalar functions, that is, functions of \(x,y,z\) since we are in \(\mathbb{R}^3\) with Cartesian coordinates.</p>
<h4>Zero-Order Tensors</h4>
<ul>
<li>They have \(n^0=1\) component.</li>
<li>They are denoted by a letter with no indices: \(T\)</li>
<li>Some examples of zero-order tensors are scalar functions, their integrals, and more generally all scalar quantities.</li>
<li>For example, the function \(f(x,y)=\ln(x)+13y\) is a zero-order tensor with component \(\ln(x)+13y.\)</li>
</ul>
<h4>First-Order Tensors</h4>
<ul>
<li>They have \(n^1=n\) components.</li>
<li>They are denoted by a letter and one index: \(\,T^i\,\) or \(\,T_i\,\) where \(\,i=1,\dots,n.\)</li>
<li>Some examples of first-order tensors are vectors, vector fields, and linear forms.</li>
<li>Vectors and vector fields are written as \(T^i=(T^1,\dots,T^n).\)</li>
<li>Linear forms are written as \(T_i=(T_1,\dots,T_n).\)</li>
<li>For example, the vector field \(F(x,y)=(-y,x)\) is a first-order tensor with components \(F^1=-y\) and \(F^2=x.\)</li>
</ul>
<h4>Second-Order Tensors</h4>
<ul>
<li>They have \(n^2=n\times n\) components.</li>
<li>They are denoted by a letter and two indices: \(\,T^{ij}\,\) or \(\,T^i_j\,\) or \(\,T_{ij}\,\) where \(\,i,j=1,\dots,n.\)</li>
<li>Some examples of second-order tensors are stress, the moment of inertia, the Kronecker delta, and the metric tensor, which we will discuss later.</li>
<li>Second-order tensors are written as \(n \times n\) matrices: \(\,T_{ij}=\begin{pmatrix}T_{11}&\dots&T_{1n}\\\vdots&\ddots&\vdots\\T_{n1}&\dots&T_{nn}\end{pmatrix}\,,\) \(\,T^i_j=\begin{pmatrix}T^1_1&\dots&T^1_n\\\vdots&\ddots&\vdots\\T^n_1&\dots&T^n_n\end{pmatrix}\,\) and \(\,T^{ij}=\begin{pmatrix}T^{11}&\dots&T^{1n}\\\vdots&\ddots&\vdots\\T^{n1}&\dots&T^{nn}\end{pmatrix}\)</li>
<li>For example, the Kronecker delta \(δ^i_j\) is a second-order tensor with components \(\begin{pmatrix}δ^1_1&\cdots&δ^1_n\\\vdots&\ddots&\vdots\\δ^n_1&\cdots&δ^n_n\end{pmatrix}=I_n\)</li>
</ul>
<h4>Higher-Order Tensors</h4>
<ul>
<li>Tensors of order \(k\) have \(n^k\) components.</li>
<li>They are denoted by a letter and \(k\) indices.</li>
</ul>
<h4>Important</h4>
<p>When we say that a tensor is written as a matrix, we do not mean that the matrix itself is a tensor.</p>
<p>Tensors are mathematical objects that are independent of the coordinate system used to describe them.</p>
<p>Thus, matrices are not the tensors themselves; they merely contain the components of a tensor.</p>
<h3 id="tensor-types">Types Of Tensors</h3>
<ul>
<li>A tensor is called <b>contravariant</b> if it is of type \((p,0)\) with \(p\neq 0,\) that is, if it has only upper indices.</li>
<li>A tensor is called <b>covariant</b> if it is of type \((0,q)\) with \(q\neq 0,\) that is, if it has only lower indices.</li>
<li>A tensor is called <b>mixed</b> if it is of type \((p,q)\) with \(p,q\neq 0,\) that is, if it has both upper and lower indices.</li>
</ul>
<h3 id="Einsteins-convention">Einstein Summation Convention</h3>
<p>The Einstein convention allows the omission of the summation symbol when, in a product of two tensors, an index appears once as an upper index and once as a lower index.</p>
<p>Thus, expressions of the form \(\displaystyle \sum_{i=1}^n x^i\,a_i\,\,\) or \(\,\,\displaystyle\sum_{i=1}^n x_i\,a^i\) can be written simply as \(x^i\,a_i\,\) or \(\,x_i\,a^i\) respectively.</p>
<p>This does not mean that the sum is removed; we simply omit the symbol \(\displaystyle \sum_{i=1}^n\) for brevity.</p>
<p>That is, \(x^i\,a_i=x^1\,a_1+x^2\,a_2+\dots+x^n\,a_n\)</p>
<p>The convention also applies to tensors with more indices, as long as there is an index that appears exactly twice in the same product.</p>
<p>For example, \(x^i_j\,a_i=x^1_j\,a_1+\dots+x^n_j\,a_n\)</p>
<p>By the same logic, a product in which the same index appears more than twice is not considered valid under the convention.</p>
<p>For example, \(x_i\,a^i\,b_i\) does not represent a sum, since the index \(i\) appears three times instead of two.</p>
<h4>Note</h4>
<p>Products in which the common index is an upper index in both tensors \((x^i\,a^i)\) or, respectively, a lower index in both \((x_i\,a_i),\) are not considered sums under the convention.</p>
<p>The fact that no summation takes place in this case does not mean that the expression is incorrect. It simply means that the Einstein convention does not apply.</p>
<hr>
<h2 id="addition-and-subtraction">Tensor Operations</h2>
<br>
<h3>Addition</h3>
<p>We can add two tensors only if they are of the same type \((p,q).\)</p>
<p>That is, if they both have the same number of indices and the indices are in the same positions.</p>
<p>For example, we can add \(A^i_j\) and \(B^i_j\,,\) but not \(T^i_j\) and \(S_{ij}\).</p>
<p>Let two tensors <span class="big">\(A^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span> and <span class="big">\(B^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span></p>
<p>Their sum is denoted by \(A+B\) and is the tensor:</p>
<p class="math-container big">\(\quad ! \quad (A+B)^{i_1,\dots,i_r}_{j_1,\dots,j_s}=A^{i_1,\dots,i_r}_{j_1,\dots,j_s}+B^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</p>
<p>For example, let \(A^{ij}\) and \(B^{ij}\) with \(i,j=1,2.\)</p>
<p>Their sum is the tensor \((A+B)^{ij}=A^{ij}+B^{ij}\) with \(i,j=1,2.\)</p>
<p>That is:</p>
<ul>
<li>\((A+B)^{11}=A^{11}+B^{11}\)</li>
<li>\((A+B)^{12}=A^{12}+B^{12}\)</li>
<li>\((A+B)^{21}=A^{21}+B^{21}\)</li>
<li>\((A+B)^{22}=A^{22}+B^{22}\)</li>
</ul>
<h4>Subtraction</h4>
<p>The same rules apply as in addition, but with \(-\) instead of \(+\).</p>
<h3 id="scalar-multiplication">Scalar Multiplication</h3>
<p>The scalar multiplication of a tensor <span class="big">\(A^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span> by a number \(c\in\mathbb{R},\) gives the tensor <span class="big">\(c\cdot A^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span>.</p>
<p>For example, let \(T_i\) with \(i=1,2,3\) and the number \(c=13\in\mathbb{R}.\)</p>
<p>Their scalar multiplication is the tensor \(S_i=13\cdot T_i\) with \(i=1,2,3.\)</p>
<p>That is:</p>
<ul>
<li>\(S_1=13\cdot T_1\)</li>
<li>\(S_2=13\cdot T_2\)</li>
<li>\(S_3=13\cdot T_3\)</li>
</ul>
<h3 id="multiplication">Multiplication</h3>
<p>We can multiply two tensors even if they are not of the same type \((p,q).\)</p>
<p>Let two tensors <span class="big">\(A^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span> and <span class="big">\(B^{k_1,\dots,k_p}_{l_1,\dots,l_q}\)</span>.</p>
<p>Their product is denoted by \(A\otimes B\) and is the tensor:</p>
<p class="math-container big">\(\quad ! \quad \big(A\otimes B\big)^{i_1,\dots,i_r,k_1,\dots,k_p}_{j_1,\dots,j_s,l_1,\dots,l_q}=A^{i_1,\dots,i_r}_{j_1,\dots,j_s} \cdot B^{k_1,\dots,k_p}_{l_1,\dots,l_q}\)</p>
<p>For example, let \(A^i_j\) and \(B^i\) with \(i,j=1,2.\)</p>
<p>First, we write \(B^i\) as \(B^k\) with \(k=1,2\) (in general, with a different index), because the index \(i\) already appears in \(A.\)</p>
<p>We do this so as not to be confused into thinking that a sum is created by the Einstein summation convention.</p>
<p>Their product is the tensor \((A\otimes B)^{i k}_j=A^i_j \cdot B^k\) with \(i,j,k=1,2.\)</p>
<p>That is:</p>
<ul>
<li>\((A\otimes B)^{11}_1=A^1_1\cdot B^1\)</li>
<li>\((A\otimes B)^{11}_2=A^1_2\cdot B^1\)</li>
<li>\((A\otimes B)^{12}_1=A^1_1\cdot B^2\)</li>
<li>\((A\otimes B)^{12}_2=A^1_2\cdot B^2\)</li>
<li>\((A\otimes B)^{21}_1=A^2_1\cdot B^1\)</li>
<li>\((A\otimes B)^{21}_2=A^2_1\cdot B^2\)</li>
<li>\((A\otimes B)^{22}_1=A^2_2\cdot B^1\)</li>
<li>\((A\otimes B)^{22}_2=A^2_2\cdot B^2\)</li>
</ul>
<h3 id="contraction">Contraction</h3>
<p>This applies only to tensors that have at least one upper index and at least one lower index.</p>
<p>The contraction of a tensor \(T\) of type \((p,q)\) is a new tensor of type \((p-1,q-1)\), denoted by \(C(T)\), and is obtained as follows:</p>
<ul>
<li>We choose one upper and one lower index of the tensor and write them using the same letter.</li>
<li>By the Einstein summation convention, this means that the resulting tensor is a sum.</li>
<li>In short, the two indices we chose will no longer be free indices, and thus the new tensor (the contraction of the original one) will have two fewer indices than the original.</li>
<li>The procedure will become clearer in the example.</li>
</ul>
<p>Let the tensor \(A^{i}_{jk}\) with \(i,j,k=1,2,3.\)</p>
<p>There are two possible contractions for this tensor.</p>
<p>One is the tensor \(C(A)_k=A^i_{ik}\), which is the sum \(C(A)_k=A^1_{1k}+A^2_{2k}+A^3_{3k}.\)</p>
<p>That is:</p>
<ul>
<li>\(C(A)_1=A^1_{11}+A^2_{21}+A^3_{31}\)</li>
<li>\(C(A)_2=A^1_{12}+A^2_{22}+A^3_{32}\)</li>
<li>\(C(A)_3=A^1_{13}+A^2_{23}+A^3_{33}\)</li>
</ul>
<p>Another one is the tensor \(c(A)_j=A^i_{ji}=A^1_{j1}+A^2_{j2}+A^3_{j3}.\)</p>
<p>That is:</p>
<ul>
<li>\(c(A)_1=A^1_{11}+A^2_{12}+A^3_{13}\)</li>
<li>\(c(A)_2=A^1_{21}+A^2_{22}+A^3_{23}\)</li>
<li>\(c(A)_3=A^1_{31}+A^2_{32}+A^3_{33}\)</li>
</ul>
<hr>
<h2 id="intro-to-coordinates-transformation">Coordinate Transformations In Linear Coordinate Systems</h2>
<p>When we discussed the order of tensors, we also talked about their components, or equivalently their coordinates.</p>
<p>Tensors as objects are independent of the coordinate system we use to describe them.</p>
<p>Their coordinates, however, obviously depend on the coordinate system and therefore change when we change systems.</p>
<p>In this section, we will see how the coordinates of tensors transform when we move from one linear coordinate system to another, which is also linear.</p>
<p>Linear coordinate systems are those in which the basis of the vector space consists of constant vectors.</p>
<p>A classic example of a linear coordinate system is the Cartesian coordinate system.</p>
<h3 id="vector-coordinates-transformation">Transformation Of Vector Coordinates</h3>
<p>Let \(V\) be an \(n\)-dimensional vector space over \(\mathbb{R}\) and \(B_V=\{e_1,\dots,e_n\}\) a basis of it.</p>
<p>Let \(v \in V\) be a vector with coordinates \(v^i,\) with \(i=1,\dots,n,\) with respect to the basis \(B_V.\)</p>
<p>That is, \(v=v^i e_i.\)</p>
<p>Let \(\tilde{B}_V=\{\tilde{e}_1,\dots,\tilde{e}_n\}\) be another basis of \(V.\)</p>
<p>We want to see how \(v\) is written with respect to the new basis \(\tilde{B}_V,\) that is, to find its new coordinates.</p>
<h4>Procedure</h4>
<p>First, we write \(\tilde{e}_1,\dots,\tilde{e}_n\) as linear combinations of \(e_1,\dots,e_n:\)</p>
<ul>
<li>\(\tilde{e}_1=A^1_1\,e_1+\dots+A^n_1\,e_n\)</li>
<li>\(\tilde{e}_2=A^1_2\,e_1+\dots+A^n_2\,e_n\)</li>
<li>\(\dots\)</li>
<li>\(\tilde{e}_n=A^1_n\,e_1+\dots+A^n_n\,e_n\)</li>
</ul>
<p>The \(A^i_j\) are numbers that we must determine.</p>
<p>We find them by substituting into the above equations the vectors \(e_i\) and \(\tilde{e}_i,\) which are known.</p>
<p>Using the coefficients \(A^i_j\) of the \(j\)-th equation as the entries of the \(j\)-th column of a matrix, we obtain the matrix \(A=\begin{pmatrix}A^1_1&\dots&A^1_n\\\vdots&\ddots&\vdots\\A^n_1&\dots&A^n_n\end{pmatrix}.\)</p>
<p>This matrix is called the <b>change-of-basis matrix</b> from the original basis \(B_V\) to the new basis \(\tilde{B}_V,\) and it is used to transform coordinates from one basis to another.</p>
<p>Specifically, for each \(i=1,\dots,n,\) the following hold:</p>
<ul>
<li class="big">\(\tilde{e}_i=A^j_i\,e_j\)</li>
<br>
<li class="big">\(e_i=(A^{-1})^j_i\,\tilde{e}_j\)</li>
<br>
<li class="big">\(\tilde{v}^i=(A^{-1})^i_j\,v^j\)</li>
<br>
<li class="big">\(v^i=A^i_j\,\tilde{v}^j\)</li>
</ul>
<p>We recall that, according to the Einstein summation convention, the index \(j\) produces a sum \(\displaystyle\sum_{j=1}^n.\)</p>
<h4>Important</h4>
<p>In the above formulas, \(\tilde{v}^i\) and \(v^i\) are the coordinates of a vector \(v.\)</p>    
<p>In contrast, in the formulas involving the basis vectors, \(\tilde{e}_i\) and \(e_i\) are the basis vectors themselves, not coordinates.</p>
<h4>Conclusion</h4>
<p>We observe that the basis vectors transform with the matrix \(A,\) while the coordinates of \(v\) transform with \(A^{-1}.\)</p>
<p>For this reason, basis vectors use lower indices, while the coordinates of \(v\) use upper indices.</p>
<p>In general, from now on remember that if something transforms with the matrix \(A,\) it will use lower indices, whereas if it transforms with \(A^{-1},\) it will use upper indices.</p>
<h4>Example</h4>
<p>In the vector space \(\mathbb{R}^2,\) one basis is \(B=\{(1,0),(0,1)\}.\)</p>
<p>Let the vector \(v=(13,5)\in\mathbb{R}^2\) with coordinates with respect to \(B.\)</p>
<p>That is, \(v^1=13\) and \(v^2=5.\)</p>
<p>We move to the coordinate system with basis \(\tilde{B}=\{(1,1),(-1,0)\}.\)</p>
<p>Let us find the coordinates of \(v\) with respect to \(\tilde{B}.\)</p>
<p class="math-container">\(\quad ! \quad (1,1)=A^1_1\cdot (1,0)+A^2_1\cdot (0,1)\iff (1,1)=(A^1_1,A^2_1)\iff \begin{cases}A^1_1=1\\A^2_1=1\end{cases}\)</p>
<p class="math-container">\(\quad ! \quad (-1,0)=A^1_2\cdot (1,0)+A^2_2\cdot (0,1)\iff (-1,0)=(A^1_2,A^2_2)\iff \begin{cases}A^1_2=-1\\A^2_2=0\end{cases}\)</p>
<p>Therefore \(A=\begin{pmatrix}1&-1\\1&0\end{pmatrix}\) and \(A^{-1}=\begin{pmatrix}0&1\\-1&1\end{pmatrix}.\)</p>
<p>We recall that the coefficients \(A^i_j\) of each equation are used as a column.</p>
<p>Using the formula \(\tilde{v}^i=(A^{-1})^i_j\,v^j,\) we find the new coordinates of \(v:\)</p>
<p class="math-container">\(\quad ! \quad\tilde{v}^1=(A^{-1})^1_j\,v^j=(A^{-1})^1_1\,v^1+(A^{-1})^1_2\,v^2=0\cdot 13+1\cdot 5=5\)</p>
<p class="math-container">\(\quad ! \quad\tilde{v}^2=(A^{-1})^2_j\,v^j=(A^{-1})^2_1\,v^1+(A^{-1})^2_2\,v^2=-1\cdot 13+1\cdot 5=-8\)</p>
<p>Thus \(\tilde{v}=(5,-8).\)</p>
<p>Using the formula \(v^i=A^i_j\,\tilde{v}^j,\) we can return to the original coordinates (to verify that no mistake was made).</p>
<p>Indeed, carrying out the calculations gives \(v^1=13\) and \(v^2=5,\) that is, \(v=(13,5).\)</p>
<h3 id="covector-coordinates-transformation">Transformation Of Coordinates Of Linear Forms</h3>
<p>At the beginning of the page, we mentioned what linear forms are, dual spaces, and the Kronecker delta. If you skipped that part, it is a good idea to review it now before continuing.</p>
<p>Let \(V\) be an \(n\)-dimensional vector space over \(\mathbb{R}\) and \(B_V=\{e_1,\dots,e_n\}\) a basis of it.</p>
<p>Let \(\tilde{B}_V=\{\tilde{e}_1,\dots,\tilde{e}_n\}\) be a new basis of \(V,\) and let \(A^i_j\) be the change-of-basis matrix from \(B_V\) to \(\tilde{B}_V.\)</p>
<p>Let \(V^*\) be the dual space defined by \(V\) and \(B_{V^*}=\{e^1,\dots,e^n\}\) the dual basis with respect to \(B_V.\)</p>
<p>Let the linear form \(ω\in V^*\) have coordinates \((ω_1,\dots,ω_n)\) with respect to the basis \(B_{V^*}.\)</p>
<p>For each \(i=1,\dots,n,\) the following hold:</p>
<ul>
<li class="big">\(\tilde{e}^i=(A^{-T})^i_j\,e^j\)</li>
<br>
<li class="big">\(e^i=A^i_j\,\tilde{e}^j\)</li>
<br>
<li class="big">\(\tilde{ω}_i=A^j_i\,ω_j\)</li>
<br>
<li class="big">\(ω_i=(A^{-T})^j_i\,\tilde{ω}_j\)</li>
</ul>
<p>We recall that, according to the Einstein summation convention, the index \(j\) produces a sum \(\displaystyle\sum_{j=1}^n.\)</p>
<p>Also, the transpose inverse matrix appears because we want the property \(\tilde{e}^i(\tilde{e}_j)=δ^i_j\) to be preserved.</p>
<h4>Important</h4>
<p>In the above formulas, \(\tilde{ω}^i\) and \(ω^i\) are the coordinates of a linear form \(ω.\)</p>    
<p>In contrast, in the formulas involving the dual basis vectors, \(\tilde{e}_i\) and \(e_i\) are the dual basis vectors themselves, not coordinates.</p>
<h4>Conclusion</h4>
<p>We observe that the linear forms of the dual basis (the \(e^i\)) transform with the matrix \(A^{-1},\) while the coordinates of \(ω\) transform with \(A.\)</p>
<p>For this reason, the linear forms of the dual basis use upper indices, while the coordinates of \(ω\) use lower indices.</p>
<h3 id="tensor-coordinates-transformation">Transformation Of Coordinates Of An Arbitrary Tensor</h3>
<p>Let \(V\) be an \(n\)-dimensional vector space over \(\mathbb{R}\) and \(B_V\) a basis of it.</p>
<p>Let the tensor <span class="big">\(T^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span> have coordinates with respect to \(B_V.\)</p>
<p>Let \(\tilde{B}_V\) be the new basis in which we want to express its coordinates.</p>
<p>The change-of-basis matrix from the original basis to the new one is \(A,\) and \(A^{-1}\) is its inverse.</p>
<p>The procedure is simple:</p>
<p>For each upper index, the matrix \(A^{-1}\) is needed once, and for each lower index, the matrix \(A\) is needed once.</p>
<p>Specifically:</p>
<p class="math-container big">\(\quad ! \quad \tilde{T}^{i_1,\dots,i_r}_{j_1,\dots,j_s}=(A^{-1})^{i_1}_{p_1} \cdot (A^{-1})^{i_2}_{p_2} \cdots (A^{-1})^{i_r}_{p_r} \cdot A^{q_1}_{j_1} \cdot A^{q_2}_{j_2} \cdots A^{q_s}_{j_s} \cdot T^{p_1,\dots,p_r}_{q_1,\dots,q_s}\)</p>
<p>As we can see, each index \(p_1,\dots,p_r,q_1,\dots,q_s,\) produces a sum according to the Einstein summation convention, since it appears once as an upper index and once as a lower index.</p>
<p>This means that there are \(r+s\) sums of the form \(\displaystyle \sum_{1}^n.\)</p>
<p>The coordinate transformation formulas for vectors and linear forms that we saw earlier follow (obviously) from this general formula. That is:</p>
<ul>
<li>Vectors are tensors with one upper index, so the matrix \(A^{-1}\) is needed once: \(\quad\tilde{T}^i=(A^{-1})^i_j\,T^j\)</li>
<li>Linear forms are tensors with one lower index, so the matrix \(A\) is needed once: \(\quad\tilde{T}_i=A^j_i\,T_j\)</li>
</ul>
<p>Let us also see an example of a tensor that has both upper and lower indices.</p>
<h4>Example</h4>
<p>In \(\mathbb{R}^2,\) we have the tensor \(T^i_j\) with \(i,j=1,2,\) which has coordinates with respect to a basis \(B.\)</p>
<p>We want to find its coordinates with respect to a new basis \(\tilde{B}.\)</p>
<p>The change-of-basis matrix from \(B\) to \(\tilde{B}\) is \(A\) with components \(\{A^1_1,A^1_2,A^2_1,A^2_2\},\) and its inverse is \(A^{-1}\) with components \(\{(A^{-1})^1_1,(A^{-1})^1_2,(A^{-1})^2_1,(A^{-1})^2_2\}.\)</p>
<p>Since the tensor has one upper and one lower index, its transformation formula is \(\tilde{T}^i_j=(A^{-1})^i_p \cdot A^q_j \cdot T^p_q.\)</p>
<p>By the Einstein summation convention, the indices \(p,q,\) which take the values \(1,2,\) each produce a sum \(\displaystyle\sum_1^2.\)</p>
<p>That is:</p>
<p class="math-container">\(\quad ! \quad \tilde{T}^i_j\quad = \quad (A^{-1})^i_1 \cdot A^1_j \cdot T^1_1\quad +\quad (A^{-1})^i_1 \cdot A^2_j \cdot T^1_2\quad +\quad (A^{-1})^i_2 \cdot A^1_j \cdot T^2_1\quad +\quad (A^{-1})^i_2 \cdot A^2_j \cdot T^2_2\)</p>
<p>Therefore, the components \(\tilde{T}^1_1,\,\tilde{T}^1_2,\,\tilde{T}^2_1,\) and \(\tilde{T}^2_2\) are computed simply by substituting the values of \(i,j\) into the formula in each case. That is:</p>
<p class="math-container">\(\quad ! \quad \tilde{T}^1_1\quad =\quad (A^{-1})^1_1 \cdot A^1_1 \cdot T^1_1\quad +\quad (A^{-1})^1_1 \cdot A^2_1 \cdot T^1_2\quad +\quad (A^{-1})^1_2 \cdot A^1_1 \cdot T^2_1\quad +\quad (A^{-1})^1_2 \cdot A^2_1 \cdot T^2_2\)</p>
<p class="math-container">\(\quad ! \quad \tilde{T}^1_2\quad =\quad (A^{-1})^1_1 \cdot A^1_2 \cdot T^1_1\quad +\quad (A^{-1})^1_1 \cdot A^2_2 \cdot T^1_2\quad +\quad (A^{-1})^1_2 \cdot A^1_2 \cdot T^2_1\quad +\quad (A^{-1})^1_2 \cdot A^2_2 \cdot T^2_2\)</p>
<p class="math-container">\(\quad ! \quad \tilde{T}^2_1\quad =\quad (A^{-1})^2_1 \cdot A^1_1 \cdot T^1_1\quad +\quad (A^{-1})^2_1 \cdot A^2_1 \cdot T^1_2\quad +\quad (A^{-1})^2_2 \cdot A^1_1 \cdot T^2_1\quad +\quad (A^{-1})^2_2 \cdot A^2_1 \cdot T^2_2\)</p>
<p class="math-container">\(\quad ! \quad \tilde{T}^2_2\quad =\quad (A^{-1})^2_1 \cdot A^1_2 \cdot T^1_1\quad +\quad (A^{-1})^2_1 \cdot A^2_2 \cdot T^1_2\quad +\quad (A^{-1})^2_2 \cdot A^1_2 \cdot T^2_1\quad +\quad (A^{-1})^2_2 \cdot A^2_2 \cdot T^2_2\)</p>
<p>In a concrete example, we would end up with numbers or scalar functions for each component, since the matrices \(A\) and \(A^{-1},\) as well as the original components of \(T^i_j,\) would be known.</p>
<p>The reason we kept the example general is to avoid the calculations, which, as you can see, would be quite a lot.</p>
<p>In general, the more indices there are and the higher the dimension of the vector space, the more computations will be required.</p>
<p>For example, if we were in \(\mathbb{R}^3,\) the indices \(i,j,p,q\) would take the values \(1,2,3,\) and thus there would be \(9\) combinations of \(p,q\) in the sums arising from the Einstein summation convention, instead of the \(4\) we have here.</p>
<hr>
<h2 id="curvilinear-coordinates">Coordinate Transformations In Curvilinear Coordinate Systems</h2>
<p>Everything we have seen so far concerns linear coordinate systems, that is, systems in which the bases consist of constant vectors.</p>
<p>In curvilinear coordinate systems, the basis vectors are not constant, but depend on the point of the vector space.</p>
<p>In such systems, the coordinates are called <b>curvilinear</b>.</p>
<p>Classic examples are polar, cylindrical, and spherical coordinates.</p>
<p>For example, when we use polar coordinates, the basis of \(\mathbb{R}^2\) is \(\{(\cos(θ),\sin(θ)),(-\sin(θ),\cos(θ))\},\) which clearly depends on \(θ\) and therefore on the point of \(\mathbb{R}^2.\)</p>
<p>If we therefore want to move from one coordinate system to another, and at least one of them is a curvilinear coordinate system, then there are some differences compared to linear coordinate systems.</p>
<p>Specifically:</p>
<h3>Coordinates Of An Arbitrary Point</h3>
<p>Let \(V\) be an \(n\)-dimensional vector space over \(\mathbb{R}\) and \(B_V\) a basis of it.</p>
<p>The coordinates of an arbitrary point in \(V,\) with respect to \(B_V,\) are \((x^1,\dots,x^n).\)</p>
<p>Let \(\tilde{B}_V\) be the new basis.</p>
<p>Since one of the two systems is a curvilinear coordinate system, the corresponding coordinates of the point in the new coordinate system will be functions of the old ones.</p>
<p>That is, not simply \(\tilde{x}^i,\) but \(\tilde{x}^i(x^1,\dots,x^n).\)</p>
<p>Similarly, the original coordinates can also be written as functions of the new ones, that is, as \(x^i(\tilde{x}^1,\dots,\tilde{x}^n).\)</p>
<p>As for the basis vectors, we have \(\tilde{e}_i=\cfrac{\partial}{\partial \tilde{x}^i} (x^1,\dots,x^n),\) for each \(i=1,\dots,n.\)</p>
<p>For example, if we are in the polar coordinate system, the coordinates \((x^1,x^2)\) of a point are simply the two numbers \((r,θ).\)</p>
<p>If we now move to Cartesian coordinates, then the coordinates \((\tilde{x}^1,\tilde{x}^2)\) will be \((x,y)\) with \(x=r\cos(θ)\) and \(y=r\sin(θ).\)</p>
<p>The inverse relations are \(r=\sqrt{x^2+y^2}\) and \(θ=\text{atan2}(y,x).\)</p>
<p>If you do not know what the function \(\text{atan2}(y,x)\) is, see <a href="/atan2/">here</a>.</p>
<h3>Change-Of-Basis Matrix</h3>
<p>The matrices \(A\) and \(A^{-1}\) that we used for coordinate transformations will no longer be matrices with constant numbers, but matrices with functions.</p>
<p>Specifically, we use the Jacobian matrix \(J=\begin{pmatrix}\cfrac{\partial\tilde{x}^1}{\partial x^1}&\dots&\cfrac{\partial\tilde{x}^1}{\partial x^n}\\\vdots&\ddots&\vdots\\\cfrac{\partial\tilde{x}^n}{\partial x^1}&\dots&\cfrac{\partial\tilde{x}^n}{\partial x^n}\end{pmatrix}\) and its inverse, \(J^{-1}=\begin{pmatrix}\cfrac{\partial x^1}{\partial\tilde{x}^1}&\dots&\cfrac{\partial x^n}{\partial\tilde{x}^1}\\\vdots&\ddots&\vdots\\\cfrac{\partial x^1}{\partial\tilde{x}^n}&\dots&\cfrac{\partial x^n}{\partial\tilde{x}^n}\end{pmatrix}.\)</p>
<p>Of course, we will write them in tensorial form, namely as:</p>
<ul>
<li>\(J^i_j=\cfrac{\partial\tilde{x}^i}{\partial x^j}\,,\,\, i,j=1,\dots,n\)</li>
<li>\((J^{-1})^j_i=\cfrac{\partial x^j}{\partial \tilde{x}^i}\,,\,\, i,j=1,\dots,n\)</li>
</ul>
<h3>Basis Transformations</h3>
<p>As we said, the basis vectors are no longer constant — they depend on the point of the vector space or the dual space, respectively.</p>
<p>If \(e_i\) are the vectors of the original basis of the vector space and \(e^i\) the vectors of the original dual basis, then:</p>
<ul>
<li class="big">\(\tilde{e}_i=\cfrac{\partial x^j}{\partial\tilde{x}^i}\,e_j\)</li>
<br>
<li class="big">\(\tilde{e}^i=\cfrac{\partial\tilde{x}^i}{\partial x^j}\,e^j\)</li>
</ul>
<h3 id="tensor-coordinates-transformation-2">Transformation Of Coordinates Of An Arbitrary Tensor</h3>
<p>The coordinate transformation formulas work with the same logic we saw in linear coordinate systems, but with the following difference:</p>
<p>For each upper index, the matrix \(J^i_j=\cfrac{\partial\tilde{x}^i}{\partial x^j}\) is needed once, and for each lower index, the matrix \((J^{-1})^j_i=\cfrac{\partial x^j}{\partial\tilde{x}^i}\) is needed once (in linear systems, the opposite was true).</p>
<p>Specifically:</p>
<p>Contravariant tensors of first order:</p>
<ul>
<li class="big">\(\tilde{T}^i=\cfrac{\partial\tilde{x}^i}{\partial x^j}\,T^j\)</li>
</ul>
<p>Covariant tensors of first order:</p>
<ul>
<li class="big">\(\tilde{T}_i=\cfrac{\partial x^j}{\partial\tilde{x}^i}\,T_j\)</li>
</ul>
<p>General form:</p>
<p class="math-container big">\(\quad ! \quad\tilde{T}^{i_1,\dots,i_r}_{j_1,\dots,j_s}=\cfrac{\partial\tilde{x}^{i_1}}{\partial x^{p_1}}\cdots\cfrac{\partial\tilde{x}^{i_r}}{\partial x^{p_r}}\cdot\cfrac{\partial x^{q_1}}{\partial\tilde{x}^{j_1}}\cdots\cfrac{\partial x^{q_s}}{\partial\tilde{x}^{j_s}}\,T^{p_1,\dots,p_r}_{q_1,\dots,q_s}\)</p>
<h4>Important</h4>
<p>We recall that the partial derivative \(\cfrac{\partial \tilde{x}^i}{\partial x^j}\) is the Jacobian matrix \(J^i_j\).</p>
<p>Therefore, \(\cfrac{\partial\tilde{x}^i}{\partial x^j}\,T^j\) is in fact written as \(J^i_j\,T^j\), and thus the index \(j\) indicates a summation.</p>
<p>The same applies to the indices \(p_1,\dots,p_m\) in the general formula.</p>
<h3>Example</h3>
<p>We are working in \(\mathbb{R}^2.\)</p>
<p>Let \(T^i\,,\,\) \(i=1,2,\) be a tensor with components \(T^1=\cos(θ)\) and \(T^2=r\) with respect to the polar coordinate system.</p>
<p>We want to find its new components after transforming to Cartesian coordinates.</p>
<p>That is:</p>
<ul>
<li>\(x^1=r\)</li>
<li>\(x^2=θ\)</li>
<li>\(\tilde{x}^1=x\)</li>
<li>\(\tilde{x}^2=y\)</li>
<li>\(x=r\cos(θ)\)</li>
<li>\(y=r\sin(θ)\)</li>
</ul>
<p class="math-container">\(\quad ! \quad J=\begin{pmatrix}\cfrac{\partial\tilde{x}^1}{\partial x^1}&\cfrac{\partial\tilde{x}^1}{\partial x^2}\\\cfrac{\partial\tilde{x}^2}{\partial x^1}&\cfrac{\partial\tilde{x}^2}{\partial x^2}\end{pmatrix}=\begin{pmatrix}\cfrac{\partial (r\cos(θ))}{\partial r}&\cfrac{\partial (r\cos(θ))}{\partial θ}\\\cfrac{\partial (r\sin(θ))}{\partial r}&\cfrac{\partial (r\sin(θ))}{\partial θ}\end{pmatrix}=\begin{pmatrix}\cos(θ)&-r\sin(θ)\\\sin(θ)&r\cos(θ)\end{pmatrix}\)</p>
<p class="math-container">\(\quad ! \quad \tilde{T}^1=\cfrac{\partial\tilde{x}^1}{\partial x^1}\,T^1+\cfrac{\partial\tilde{x}^1}{\partial x^2}\,T^2=\cos(θ)\cos(θ)-rr\sin(θ)\iff\tilde{T}^1=\cos^2(θ)-r^2\sin(θ)\)</p>
<p class="math-container">\(\quad ! \quad\tilde{T}^2=\cfrac{\partial\tilde{x}^2}{\partial x^1}\,T^1+\cfrac{\partial\tilde{x}^2}{\partial x^2}\,T^2=\cos(θ)\sin(θ)+rr\cos(θ)\iff\tilde{T}^2=\cos(θ)\sin(θ)+r^2\cos(θ)\)</p>
<p>These are the components of the tensor with respect to the Cartesian coordinate system, but still expressed using the symbols \(r,θ.\)</p>
<p>To rewrite them using \(x,y,\) we simply make the following substitutions:</p>
<ul>
<li>\(r=\sqrt{x^2+y^2}\)</li>
<li>\(\cos(θ)=\cfrac{x}{r}=\cfrac{x}{\sqrt{x^2+y^2}}\)</li>
<li>\(\sin(θ)=\cfrac{y}{r}=\cfrac{y}{\sqrt{x^2+y^2}}\)</li>
</ul>
<p>Thus:</p>
<p class="math-container">\(\quad ! \quad\tilde{T}^1=\cfrac{x^2}{x^2+y^2}-\cfrac{yx^2+y^3}{\sqrt{x^2+y^2}}\)</p>
<p class="math-container">\(\quad ! \quad\tilde{T}^2=\cfrac{xy}{x^2+y^2}+\cfrac{x^3+xy^2}{\sqrt{x^2+y^2}}\)</p>
<hr>
<h2 id="multilinear-maps">Definition Of Tensors</h2>
<p>Up to this point, we have seen what tensors are and how they behave.</p>
<p>It is now time to look at their formal definition.</p>
<p>First, however, we need to explain what multilinear maps and tensor product spaces are.</p>
<h3>Multilinear Maps</h3>
<p>Let \(V_1,\dots,V_n\) and \(W\) be vector spaces over \(\mathbb{R}.\)</p>
<p>A map \(T:V_1\times\cdots\times V_n\to W\) is called <b>multilinear</b> if it is linear with respect to each \(V_i\).</p>
<p>That is, for all \(a,b\in\mathbb{R}\) and all \(x_i,y_i,v_i\in V_i\) with \(i=1,\dots,n\), the following hold:</p>
<p class="math-container">\(\quad ! \quad T(ax_1+by_1,v_2,\dots,v_n)=a\cdot T(x_1,v_2,\dots,v_n)+b\cdot T(y_1,v_2,\dots,v_n)\)</p>
<p class="math-container">\(\quad ! \quad T(v_1,ax_2+by_2,\dots,v_n)=a\cdot T(v_1,x_2,\dots,v_n)+b\cdot T(v_1,y_2,\dots,v_n)\)</p>
<p class="math-container">\(\quad ! \quad \vdots\)</p>
<p class="math-container">\(\quad ! \quad T(v_1,\dots,v_{n-1},ax_n+by_n)=a\cdot T(v_1,\dots,v_{n-1},x_n)+b\cdot T(v_1,\dots,v_{n-1},y_n)\)</p>
<h4>Examples</h4>
<p>We have already seen linear forms \(ω:V\to\mathbb{R}.\)</p>
<p>A bilinear form is the metric tensor \(g:V\times V\to\mathbb{R}\), which we will study later.</p>
<p>A trilinear form is the Levi-Civita symbol, defined as \(ε_{ijk}=δ^{1\,2\,3}_{i\,j\,k},\) where \(δ\) is the generalized Kronecker delta.</p>
<p>Soon, we will use multilinear maps to define tensors.</p>
<h3 id="tensor-product-spaces">Tensor Product Spaces</h3>
<p>Let \(V,W\) be vector spaces over \(\mathbb{R}.\)</p>
<p>We define the <b>tensor product</b> of \(V,W\) as the pair \((V\otimes W,\otimes),\) where \(\otimes\) is a bilinear map \(\otimes:V\times W\to V\otimes W\) such that for every bilinear map \(f:V\times W\to U,\) there exists a unique linear map \(g:V\otimes W\to U\) such that \(f(v,w)=g(v\otimes w).\)</p>
<p>The space \(V\otimes W\) is called the <b>tensor product space</b>.</p>
<p>In simple terms, \(V\otimes W\) is a space in which every bilinear map \(T:V\times W\to U\) corresponds to a linear map \(T:V\otimes W\to U.\)</p>
<p>Using more vector spaces \(V_1,\dots,V_n,\) we define in the same way the tensor product space \(V_1\otimes\cdots\otimes V_n\), in which every multilinear map corresponds to a linear map.</p>
<h3 id="tensors-definition">Definition Of Tensors</h3>
<p>There are two equivalent definitions.</p>
<h4>\(1)\) Definition Of Tensors As Multilinear Maps</h4>
<p>A tensor of type \((p,q)\) is a multilinear map \(T:{(V^*)}^p \times V^q\to\mathbb{R}.\)</p>
<p>More explicitly:</p>
<p class="math-container">\(\quad ! \quad T:\underbrace{V^*\times\cdots\times V^*}_{p\text{ times}}\times\underbrace{V\times\cdots\times V}_{q\text{ times}}\to\mathbb{R}\)</p>
<h5>Examples</h5>
<p>All scalar quantities are tensors of type \((0,0)\) and are written as \(T:\{\text{point}\}\to\mathbb{R}.\)</p>
<p>Linear forms are tensors of type \((0,1)\) and are written as \(ω:V\to\mathbb{R}.\)</p>
<p>Vectors are tensors of type \((1,0)\) and can be written as \(v:V^*\to\mathbb{R}.\)</p>
<p>The inner product is a tensor of type \((0,2)\) and is written as \(\langle\,\rangle:V\times V\to\mathbb{R}.\)</p>
<p>And many others.</p>
<h4>\(2)\) Definition Of Tensors As Elements Of A Tensor Product Space</h4>
<p>A tensor of type \((p,q)\) is an element of the tensor product space \(V^{\otimes p}\otimes {(V^*)}^{\otimes q}\), which can be written more explicitly as \(\underbrace{V\otimes\cdots\otimes V}_{p\text{ times}}\otimes\underbrace{V^*\otimes\cdots\otimes V^*}_{q\text{ times}}\).</p>
<h5>Examples</h5>
<p>Linear forms are tensors of type \((0,1)\) and are elements of \(V^*\).</p>
<p>Bilinear forms are tensors of type \((0,2)\) and are elements of \(V^*\otimes V^*\).</p>
<p>Vectors are tensors of type \((1,0)\) and are elements of \(V\).</p>
<p>The inner product is a tensor of type \((0,2)\) and is an element of \(V^*\otimes V^*\).</p>
<p>Similarly, by defining the appropriate tensor product space, various tensors are defined as its elements.</p>
<h4>Note</h4>
<p>We could have mentioned the two definitions at the beginning of the page together with the basic properties of tensors.</p>
<p>However, starting the course by saying that tensors are multilinear maps, or elements of tensor product spaces, would not help in understanding tensors and would most likely discourage you from continuing.</p>
<p>For this reason, we chose to begin with a description of tensors, their properties, their coordinate transformations, etc., and to leave the definitions for now.</p>
<h3 id="how-to-check-if-something-is-a-tensor">How To Check Whether A Mathematical Object Is A Tensor</h3>
<p>In practice, we rarely use the above definitions to check whether something is a tensor.</p>
<p>It is easier to use the following statement:</p>
<p><i>"A mathematical object is a tensor \(\iff\) under any change of coordinate system, its components transform according to the tensor coordinate transformation laws."</i></p>
<h4>Example</h4>
<p>We will check whether vectors are tensors.</p>
<p>Let \(V\) be a vector space and let \(v\in V.\)</p>
<p>In a coordinate system \((x^1,\dots,x^n),\) the vector is written as \(v=v^i\,e_i,\) where \(e_i\) are the basis vectors and \(v^i\) are the components of the vector.</p>
<p>We change the coordinate system to \((\tilde{x}^1,\dots,\tilde{x}^n).\)</p>
<p>As we have seen, the basis vectors transform as \(\tilde{e}_{i}=\cfrac{\partial x^j}{\partial \tilde{x}^{i}}\,e_j\)</p>
<p>Since we want the vector \(v\) to remain the same, as a geometric object, we must have \(\tilde{v}^i\,\tilde{e}_i=v^i\,e_i\)</p>
<p>Substituting into this relation the transformation of the bases, we obtain \(\tilde{v}^i \,\cfrac{\partial x^j}{\partial \tilde{x}^{i}}\,e_j=v^i\,e_i\)</p>
<p>Both \(i\) and \(j\) are summation indices and take the same values, therefore we may rename them without changing the meaning of the relation.</p>
<p>That is, we can write \(\tilde{v}^i \,\cfrac{\partial x^j}{\partial \tilde{x}^{i}}\,e_j=v^j\,e_j\)</p>
<p>Thus, the coefficients of \(e_j\) must be equal. That is, \(\,\,\tilde{v}^i \,\cfrac{\partial x^j}{\partial \tilde{x}^i}=v^j\)</p>
<p>We multiply by the inverse of \(\cfrac{\partial x^j}{\partial \tilde{x}^i},\) namely \(\cfrac{\partial \tilde{x}^k}{\partial x^j},\) and obtain \(\,\,\tilde{v}^i \,\cfrac{\partial \tilde{x}^k}{\partial x^j}\,\cfrac{\partial x^j}{\partial \tilde{x}^i}=\cfrac{\partial \tilde{x}^k}{\partial x^j}\,v^j\)</p>
<p>The quantity \(\cfrac{\partial \tilde{x}^k}{\partial x^j}\,\cfrac{\partial x^j}{\partial \tilde{x}^i}\) is the well-known chain rule from calculus. That is, it is equal to \(\cfrac{\partial\tilde{x}^k}{\partial\tilde{x}^i}.\)</p>
<p>The \(\tilde{x}^k\) and \(\tilde{x}^i,\) with \(i,k=1,\dots,n,\) are the coordinates of the new coordinate system.</p>
<p>If, for example, these are polar coordinates, they will be \(r,θ.\) If they are Cartesian coordinates in \(\mathbb{R}^3,\) they will be \(x,y,z.\)</p>
<p>Therefore, since we are dealing with the coordinates of a system and not with functions, the partial derivative \(\cfrac{\partial\tilde{x}^k}{\partial\tilde{x}^i}\) can be equal either to \(0,\) if \(k\neq i,\) or to \(1,\) if \(k=i.\)</p>
<p>Thus \(\cfrac{\partial\tilde{x}^k}{\partial\tilde{x}^i}=δ^k_i\)</p>
<p>Returning to the equality above, we have \(\,\,\tilde{v}^i \,δ^k_i=\cfrac{\partial \tilde{x}^k}{\partial x^j}\,v^j \iff \tilde{v}^k=\cfrac{\partial \tilde{x}^k}{\partial x^j}\,v^j\)</p>
<p>This is exactly the transformation law of a tensor of type \((1,0).\)</p>
<p>Therefore, vectors are contravariant tensors of first order.</p>
<hr>
<h2 id="metric-tensor">Metric Tensor</h2>
<p>Let \(V\) be an \(n\)-dimensional vector space and \(B_V=\{e_1,\dots,e_n\}\) a basis of it.</p>
<p>The <b>metric tensor</b> is a symmetric bilinear form \(g:V\times V\to\mathbb{R}\) with \(g_{ij}=g(e_i,e_j)=e_i \cdot e_j\)</p>
<h4>Properties</h4>
<ul>
<li>It is a covariant tensor of second order (type \((0,2)\)).</li>
<li>It is written as an \(n\times n\) matrix.</li>
<li>It is symmetric, hence \(g_{ij}=g_{ji}\)</li>
<li>Its inverse is the contravariant tensor \(g^{ij}\)</li>
</ul>
<h3 id="examples-of-metric-tensor">Examples</h3>
<p>\(1)\) In \(\mathbb{R}^2\) and with respect to the basis \(\{(0,1),(1,0)\},\) the metric tensor is:</p>
<ul>
<li>\(g_{11}=(0,1)\cdot (0,1)=0+1=1\)</li>
<li>\(g_{12}=(0,1)\cdot (1,0)=0+0=0\)</li>
<li>\(g_{21}=(1,0)\cdot (0,1)=0+0=0\)</li>
<li>\(g_{22}=(1,0)\cdot (1,0)=1+0=1\)</li>
</ul>
<p>That is, \(g_{ij}=\begin{pmatrix}1&0\\0&1\end{pmatrix}\)</p>
<p>In general, in \(\mathbb{R}^n\) and as long as we use Cartesian coordinates, we have \(g_{ij}=δ_{ij}\)</p>
<p>\(2)\) In \(\mathbb{R}^2\) using polar coordinates, \(x=r\cos(θ)\) and \(y=r\sin(θ),\) hence \((x,y)=(r\cos(θ),r\sin(θ)),\) we have:</p>
<p class="math-container">\(\quad ! \quad e_1=\cfrac{\partial}{\partial r} (r\cos(θ),r\sin(θ))=(\cos(θ),\sin(θ))\)</p>
<p class="math-container">\(\quad ! \quad e_2=\cfrac{\partial}{\partial θ} (r\cos(θ),r\sin(θ))=(-r\sin(θ),r\cos(θ))\)</p>
<p class="math-container">\(\quad ! \quad g_{11}=e_1\cdot e_1=\cos^2(θ)+\sin^2(θ)=1\)</p>
<p class="math-container">\(\quad ! \quad g_{12}=g_{21}=e_1\cdot e_2=-r\cos(θ)\sin(θ)+r\cos(θ)\sin(θ)=0\)</p>
<p class="math-container">\(\quad ! \quad g_{22}=e_2\cdot e_2=r^2\sin^2(θ)+r^2\cos^2(θ)=r^2\)</p>
<p>Thus \(g_{ij}=\begin{pmatrix}1&0\\0&r^2\end{pmatrix}\)</p>
<p>\(3)\) On a regular surface in \(\mathbb{R}^3,\) that is, the ones we discuss in the course <a href="/differential-geometry-I/">"Differential Geometry <span class="consolas">I</span>"</a>, the metric tensor is the first fundamental form.</p>
<p>That is, \(g_{ij}=\begin{pmatrix}E&F\\F&G\end{pmatrix}\)</p>
<h3 id="uses-of-metric-tensor">Uses Of The Metric Tensor</h3>
<p>There are many uses. Some of them are:</p>
<p>\(1)\) Length of a vector: \(\,\,\|v\|=\sqrt{g_{ij}\,v^i\,v^j}\)</p>
<p>\(2)\) Line element:</b> \(\,\, ds^2=g_{ij}\,dx^i\,dx^j\)</p>
<h4>\(3)\) Raising And Lowering Indices</h4>
<p>Using the metric tensor, we can convert an upper index into a lower one and vice versa.</p>
<p>This is helpful in various situations, for example when converting contravariant or covariant tensors into mixed tensors so that we can perform their contraction.</p>
<p>Let the tensors be \(T_i,\) \(S^i,\) \(A^i_j,\) the metric tensor \(g_{ij}\) and its inverse \(g^{ij}.\)</p>
<p>We have:</p>
<ul>
<li>\(T^i=g^{ij}\,T_j\)</li>
<li>\(S_i=g_{ij}\,S^j\)</li>
<li>\(A^{ij}=g^{jk}\,A^i_k\)</li>
<li>\(A_{ij}=g_{ik}\,A^k_j\)</li>
</ul>
<p>There are formulas for tensors of arbitrary order, but we will not go further into this in this course.</p>
<p>We simply wanted to mention that this possibility exists.</p>
<h5>Important</h5>
<p>As we said before, in \(\mathbb{R}^n\) with Cartesian coordinates, the metric tensor is \(g_{ij}=δ_{ij}\)</p>
<p>Therefore, for the components \(v^i\) of a vector, we have \(v_i=δ_{ij}\,v^j,\) that is, \(v_i=v^i.\)</p>
<p>For this reason, in other courses, it is customary to use lower indices for vectors as well.</p>
<p>And since before seeing this course you were most likely always working in \(\mathbb{R}^n\) with Cartesian coordinates, it is natural that you became accustomed to using lower indices for everything.</p>
<p>This is also the reason why tensor analysis usually seems difficult at first: because the notation we were used to seeing for years changes.</p>
<p>Of course, if you have reached this point on the page, you have probably overcome any initial difficulties you may have had.</p>
<p>Finally, the fact that in this case we use only lower indices means that Einstein’s summation convention may also apply in cases where the repeated index appears both times as a lower index or both times as an upper index (for example \(a_i\,v_i\)).</p>
<p>To avoid confusion, we will continue to use the standard notation in the remainder of the course, even if in some example we are working in \(\mathbb{R}^n\) with Cartesian coordinates.</p>
<hr>
<h2 id="ordinary-partial-differentiation">Differentiation Of Tensors</h2>
<p>We will examine two types of differentiation: the ordinary one and the covariant one.</p>
<p>Which of the two is used depends on the tensor and the space in which it is defined.</p>
<h3>Ordinary Differentiation</h3>
<p>It applies to:</p>
<ul>
<li>Scalar functions (tensors of order zero) in any space and regardless of the coordinate system.</li>
<li>Any tensor defined in the space \(\mathbb{R}^n\) using a linear coordinate system.</li>
</ul>
<p>In the first case, we have scalar functions and differentiation is performed using partial derivatives (or a single derivative if it is a function of one variable), as we know from calculus courses.</p>
<p>In the second case:</p>
<p>We are in \(\mathbb{R}^n\) and use Cartesian coordinates \((x^1,\dots,x^n).\)</p>
<p>Let the tensor be <span class="big">\(T^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span></p>
<p>For each \(x^i,\) the partial derivative \(\cfrac{\partial}{\partial x^i}\) is denoted by \(\partial_i\)</p>
<p>Thus, <span class="big">\(\partial_i\,T^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span> denotes the derivative of the tensor \(T\) with respect to the coordinate \(x^i\) and is computed by differentiating each component of \(T\) with \(\partial_i\)</p>
<h4>Example</h4>
<p>We are in \(\mathbb{R}^2\) with Cartesian coordinates \((x,y).\)</p>
<p>Let the tensor be \(V=(xy^2,\cos(x))\) (a vector field).</p>
<p>That is:</p>
<ul>
<li>\(x^1=x\)</li>
<li>\(x^2=y\)</li>
<li>\(V^1=xy^2\)</li>
<li>\(V^2=\cos(x)\)</li>
<li>\(\partial_1=\cfrac{\partial}{\partial x}\)</li>
<li>\(\partial_2=\cfrac{\partial}{\partial y}\)</li>
</ul>
<p>Therefore:</p>
<ul>
<li>\(\partial_1\,V^1=\partial_1\,xy^2=y^2\)</li>
<li>\(\partial_1\,V^2=\partial_1\,\cos(x)=-\sin(x)\)</li>
<br>
<li>\(\partial_2\,V^1=\partial_2\,xy^2=2xy\)</li>
<li>\(\partial_2\,V^2=\partial_2\,\cos(x)=0\)</li>
</ul>
<p>Thus \(\,\,\partial_1\,V=(y^2,-\sin(x))\,\,\) and \(\,\,\partial_2\,V=(2xy,0).\)</p>
<p>As you can see, the procedure is familiar (you have probably differentiated a vector field at some point); the only thing that changes is the notation.</p>
<p>The procedure is the same for higher-order tensors; there will simply be more components.</p>
<h3 id="covariant-differentiation">Covariant Differentiation</h3>
<p>It applies to:</p>
<ul>
<li>Any tensor defined in \(\mathbb{R}^n\) using a curvilinear coordinate system.</li>
<li>Any tensor that is not defined in \(\mathbb{R}^n.\)</li>
</ul>
<p>First of all, the reason why ordinary differentiation does not work in these cases is that the bases in these spaces are not constant, but change from point to point.</p>
<p>The differentiation rules we have used so far assumed that the bases are constant, and therefore they do not work correctly here.</p>
<p>For this reason, we use the so-called <b>covariant differentiation.</b></p>
<p>Before explaining how it works, it should be noted that it has NOTHING to do with covariant tensors. It applies to any tensor, whether covariant, contravariant, or mixed.</p>
<h4>Procedure</h4>
<p>We are in some space, either \(\mathbb{R}^n\) or not, and we use coordinates \((x^1,\dots,x^n).\)</p>
<p>Let the tensor be <span class="big">\(T^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span></p>
<p>The covariant derivative of the tensor \(T\) with respect to the coordinate \(x^k,\) denoted by <span class="big">\(\nabla_k\,T^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span>, is equal to its ordinary partial derivative with respect to \(x^k,\) namely <span class="big">\(\partial_k\,T^{i_1,\dots,i_r}_{j_1,\dots,j_s},\)</span> plus some extra terms that “correct” the ordinary partial derivative, which ignores the change of basis.</p>
<p>Specifically:</p>
<ul>
<li>For each upper index \(i_a,\) that is for each \(a=1,\dots,r,\) the term <span class="big">\(\,\,Γ^{i_a}_{k\,l}\,T^{i_1,\dots,l,\dots,i_r}_{j_1,\dots,j_s}\,,\,\)</span> is added, where \(l\) replaces \(i_a\) among the upper indices of \(T\) and creates a sum due to Einstein’s summation convention.</li>
<li>For each lower index \(j_b,\) that is for each \(b=1,\dots,s,\) the term <span class="big">\(\,\,Γ^{l}_{k\,j_b}\,T^{i_1,\dots,i_r}_{j_1,\dots,l,\dots,j_s}\,,\,\)</span> is subtracted, where \(l\) replaces \(j_b\) among the lower indices of \(T\) and creates a sum due to Einstein’s summation convention.</li>
<li>The symbols \(Γ^k_{ij}\) are called Christoffel symbols, and their values depend on the geometric structure of the space we are working in. There are therefore no universal formulas for computing them that apply to every space. For example, in the course <a href="/differential-geometry-I/">"Differential Geometry <span class="consolas">I</span>"</a> (in the section “Gauss and Weingarten Formulas”), we saw how to compute them in the case of a regular surface in \(\mathbb{R}^3.\)</li>
</ul>
<p>That is:</p>
<h5>Zero-Order Tensors</h5>
<p>The tensor \(T\) has no indices, and therefore no terms are added to or subtracted from the ordinary partial derivative.</p>
<ul><li>\(\nabla_k\,T=\partial_k\,T\)</li></ul>
<p>That is, the covariant derivative coincides with the ordinary derivative.</p>
<h5>First-Order Contravariant Tensors</h5>
<p>The tensor is of the form \(T^i,\) and therefore one term must be added to the ordinary partial derivative.</p>
<ul><li>\(\nabla_k\,T^i=\partial_k\,T^i+Γ^i_{k\,l}\,T^l\)</li></ul>
<p>We recall that the index \(l\) is summed over.</p>
<h5>First-Order Covariant Tensors</h5>
<p>The tensor is of the form \(T_i,\) and therefore one term must be subtracted from the ordinary partial derivative.</p>
<ul><li>\(\nabla_k\,T_i=\partial_k\,T_i-Γ^l_{k\,i}\,T_l\)</li></ul>
<p>We recall that the index \(l\) is summed over.</p>
<h5>Any Tensor</h5>
<p>The tensor has the form <span class="big">\(T^{i_1,\dots,i_r}_{j_1,\dots,j_s}\)</span>, and therefore we must add \(r\) terms and subtract \(s\) terms from the ordinary partial derivative.</p>
<p class="math-container big">\(\quad ! \quad \nabla_k\,T^{i_1,\dots,i_r}_{j_1,\dots,j_s}=\partial_k\,T^{i_1,\dots,i_r}_{j_1,\dots,j_s}+\underbrace{Γ^{i_1}_{kl}\,T^{l,i_2,\dots,i_r}_{j_1,\dots,j_s}+Γ^{i_2}_{kl}\,T^{i_1,l,\dots,i_r}_{j_1,\dots,j_s}+\cdots+Γ^{i_r}_{kl}\,T^{i_1,\dots,i_{r-1},l}_{j_1,\dots,j_s}}_{r \text{ terms, where } l \text{ replaces each upper index } i_a}-\underbrace{Γ^{l}_{kj_1} T^{i_1,\dots,i_r}_{l,j_2,\dots,j_s}-Γ^{l}_{kj_2} T^{i_1,\dots,i_r}_{j_1,l,\dots,j_s}-\cdots-Γ^{l}_{kj_s} T^{i_1,\dots,i_r}_{j_1,\dots,j_{s-1},l}}_{s \text{ terms, where } l \text{ replaces each lower index } j_b}\)</p>
<p>It is true that the formula looks intimidating because it is full of different symbols, but in practice you will never work with it in its fully general form.</p>
<h5>Example</h5>
<p>We are in \(\mathbb{R}^2\) using polar coordinates \((r,θ).\)</p>
<p>Let the tensor be \(T^i_j=\begin{pmatrix}1&0\\0&r\end{pmatrix}\)</p>
<p>That is:</p>
<ul>
<li>\(x^1=r\)</li>
<li>\(x^2=θ\)</li>
<li>\(T^1_1=1\)</li>
<li>\(T^1_2=0\)</li>
<li>\(T^2_1=0\)</li>
<li>\(T^2_2=r\)</li>
<li>\(\partial_1=\cfrac{\partial}{\partial r}\)</li>
<li>\(\partial_2=\cfrac{\partial}{\partial θ}\)</li>
</ul>
<p>In \(\mathbb{R}^2\) with polar coordinates, the Christoffel symbols are computed using the formula \(Γ^k_{ij}=\cfrac{1}{2}\,g^{kl}\,\biggr(\partial_i\,g_{jl}+\partial_j\,g_{il}-\partial_l\,g_{ij}\biggr),\) with \(i,j,k,l=1,2,\) where \(g\) is the metric tensor and, as we saw earlier, it is \(g_{ij}=\begin{pmatrix}1&0\\0&r^2\end{pmatrix}.\)</p>
<p>Its inverse is \(g^{ij}=\begin{pmatrix}1&0\\0&\cfrac{1}{r^2}\end{pmatrix}\)</p>
<p>Performing the calculations, we find:</p>
<ul>
<li>\(Γ^1_{22}=-r\)</li>
<li>\(Γ^2_{12}=Γ^2_{21}=\cfrac{1}{r}\)</li>
<li>All the remaining ones are equal to \(0.\)</li>
</ul>
<p>We mentioned these because they are needed for the calculations.</p>
<p>Now, let us compute the covariant derivative \(\nabla_1\,T^i_j\)</p>
<p>The tensor has one upper and one lower index, therefore:</p>
<p>\(\nabla_1\,T^i_j=\partial_1\,T^i_j+Γ^i_{1l}\,T^l_j-Γ^l_{1j}\,T^i_l\)</p>
<p>The index \(l\) takes the values \(1,2,\) just like \(i,j,\) so performing the sums:</p>
<p class="math-container">\(\quad ! \quad \nabla_1\,T^i_j=\partial_1\,T^i_j+Γ^i_{11}\,T^1_j+Γ^i_{12}\,T^2_j-Γ^1_{1j}\,T^i_1-Γ^2_{1j}\,T^i_2\)</p>
<p>That was all. Now we simply compute each component by substituting the values of \(i,j\) in each case:</p>
<p class="math-container">\(\quad ! \quad \nabla_1\,T^1_1=\partial_1\,T^1_1+Γ^1_{11}\,T^1_1+Γ^1_{12}\,T^2_1-Γ^1_{11}\,T^1_1-Γ^2_{11}\,T^1_2=0+0\cdot 1+0\cdot 0-0\cdot 1-0\cdot 0=0\)</p>
<p class="math-container">\(\quad ! \quad \nabla_1\,T^1_2=\partial_1\,T^1_2+Γ^1_{11}\,T^1_2+Γ^1_{12}\,T^2_2-Γ^1_{12}\,T^1_1-Γ^2_{12}\,T^1_2=0+0\cdot 0+0\cdot r+0\cdot 1+\cfrac{1}{r}\cdot 0=0\)</p>
<p class="math-container">\(\quad ! \quad \nabla_1\,T^2_1=\partial_1\,T^2_1+Γ^2_{11}\,T^1_1+Γ^2_{12}\,T^2_1-Γ^1_{11}\,T^2_1-Γ^2_{11}\,T^2_2=0+0\cdot 1+\cfrac{1}{r}\cdot 0-0\cdot 0-0\cdot r=0\)</p>
<p class="math-container">\(\quad ! \quad \nabla_1\,T^2_2=\partial_1\,T^2_2+Γ^2_{11}\,T^1_2+Γ^2_{12}\,T^2_2-Γ^1_{12}\,T^2_1-Γ^2_{12}\,T^2_2=1+0\cdot 0+\cfrac{1}{r}\cdot r+0\cdot 0-\cfrac{1}{r}\cdot r=1\)</p>
<p>In this example, it happened that all components were constant.</p>
<p>Obviously this is not required; they could be scalar functions, just as in ordinary differentiation.</p>
<p>The procedure is the same for \(\nabla_2\,T^i_j\)</p>
<hr>
<h2 id="tensor-form-of-gradient">Tensor Form Of Well-Known Math Concepts</h2>
<p>We are in \(\mathbb{R}^n\) and use Cartesian coordinates.</p>
<h3>Tensor Form Of The Gradient (Nabla)</h3>
<p>Let \(f(x^1,\dots,x^n)\) be a scalar function (a zero-order tensor).</p>
<p>The gradient of \(f\) is defined as \(\nabla f=\left(\cfrac{\partial f}{\partial x^1},\dots,\cfrac{\partial f}{\partial x^n}\right)\)</p>
<p>The gradient of \(f\) is a first-order covariant tensor and is written as \((\nabla f)_i=\partial_i f\)</p>
<h3 id="tensor-form-of-divergence">Tensor Form Of The Divergence</h3>
<p>Let \(F(x^1,\dots,x^n)=(F^1,\dots,F^n)\) be a vector field (a first-order tensor).</p>
<p>The divergence of \(F\) is defined as \(\displaystyle \nabla \cdot F=\sum_{i=1}^n \cfrac{\partial F^i}{\partial x^i}\)</p>
<p>The divergence of \(F\) is a zero-order tensor and is written as \(\nabla \cdot F=\partial_i\,F^i\)</p>
<h3 id="tensor-form-of-curl">Tensor Form Of The Curl</h3>
<p>Let \(F(x^1,x^2,x^3)=(F^1,F^2,F^3)\) be a vector field (a first-order tensor).</p>
<p>The curl of \(F\) is given by \(\nabla\times F=\begin{vmatrix}\vec{i}&\vec{j}&\vec{k}\\\cfrac{\partial}{\partial x^1}&\cfrac{\partial}{\partial x^2}&\cfrac{\partial}{\partial x^3}\\F^1&F^2&F^3\end{vmatrix}\)</p>
<p>The curl of \(F\) is a first-order contravariant tensor and is written as \((\nabla\times F)^i=ε^{ijk}\,\partial_j\,F_k\,,\) where \(ε^{ijk}\) is the Levi–Civita symbol.</p>
<h4>Note</h4>
<p>To lower the index \(k,\) that is, to use \(F_k\) instead of \(F^k,\) we must use the formula \(F_k=g_{kj}\,F^j\)</p>
<p>However, since we are in \(\mathbb{R}^3\) with Cartesian coordinates, we have \(g_{ij}=δ_{ij}\) and therefore \(F_k=F^k.\)</p>
<h3 id="tensor-form-of-Laplacian">Tensor Form Of The Laplacian</h3>
<br>
<h4>Scalar Functions</h4>
<p>Let \(f(x^1,\dots,x^n)\) be a scalar function (a zero-order tensor).</p>
<p>The Laplacian of \(f\) is defined as \(\displaystyle\nabla^2 f=\sum_{i=1}^n\cfrac{\partial^2 f}{\partial x^i \partial x^i}\)</p>
<p>The Laplacian of \(f\) is a zero-order tensor and is written as \(\nabla^2 f=δ^{ij}\,\partial_i\,\partial_j\,f\)</p>
<h4>Vector Fields</h4>
<p>Let \(F(x^1,\dots,x^n)=(F^1,\dots,F^n)\) be a vector field (a first-order tensor).</p>
<p>The Laplacian of \(F\) is defined as \(\nabla^2 F=(\nabla^2 F^1,\dots,\nabla^2 F^n)\) (recall that the \(F^i\) are scalar functions).</p>
<p>The Laplacian of \(F\) is a first-order contravariant tensor and is written as \(\displaystyle (\nabla^2 F)^k=δ^{ij}\,\partial_i\,\partial_j\,F^k\)</p>
</main>
<footer>
<hr>
<br>
<a href="#" id="back-to-top">Back to top</a>
<div id="end-of-page">
<div class="end-of-page-component" id="technical-issues">
<h2>Technical Issues</h2>
<p>If math symbols are not displayed correctly, please visit the following page for assistance:</p>
<a href="/help/" class="doNotStyle">Help</a>
</div>
<div class="end-of-page-component">
<h2>Useful Links</h2>
<a href="#" class="doNotStyle">Back to top</a>
<a href="/" class="doNotStyle">Homepage</a>
<a href="/library/" class="doNotStyle">Library</a>
</div>
<div class="end-of-page-component" id="contact-us">
<h2>Contact Us</h2>
<div><b>Email:</b> contact@freemath13.com</div>
<div><b>instagram:</b> freemath13.en</div>
</div>
</div>
<div id="ending">&copy; FreeMath13. All rights reserved</div>
</footer>
</body>
</html>